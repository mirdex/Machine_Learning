{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 中文文字分類\n",
    "\n",
    "## ```Da-Wei Chiang```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ```文字採擷(Text Mining)```\n",
    "\n",
    "- 意義 : 從非結構化資料中找出使用者感興趣或有用模式的過程\n",
    "- 定義 : 從大量文字資料中取出事前未知的、可了解的、最後可用的知識的過程\n",
    "- 文字採擷分為以下幾個領域 : \n",
    "    - 搜索和資訊檢索(IR):儲存和文字文件的檢索，如:搜尋引擎、關鍵字搜尋\n",
    "    - 文字分群 : 使用分群，對詞彙、片段、段落進行歸類\n",
    "    - 文字分類 : 對詞彙、片段、段落進行歸類，在資料採擷分類方法上進行建模\n",
    "    - Web採擷 : 網際網路上進行資料和文字採擷\n",
    "    - 資訊取出(IE) : 從非結構化資料中識別與分析有關的事實和關係\n",
    "    - 自然語言處理(NLP) : 從語言、語法的角度發現語言最本質的結構所產生的意義\n",
    "    - 概念分析 : 將單字和子句按語意分成幾個相似的組"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ```文字分類```\n",
    "\n",
    "- 文字分類的方法分為以下兩種\n",
    "    - 模式系統(專家評估)\n",
    "    - 分類模型(機器學習)\n",
    "- 文字分類的應用\n",
    "    - 文字檢索、垃圾郵件過濾、網頁分層目錄...等等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ```中文文字分類步驟```\n",
    "\n",
    "- 中文的文字分類流程，主要包含以下幾個步驟\n",
    "    - 前置處理 : 去除文字雜訊，如文字格式轉換、去除HTML標籤..等等\n",
    "    - 分詞 : 為中文分詞，並去除停用詞 \n",
    "    - 建置詞的向量空間 : 統計文字詞頻，產生文字的向量空間\n",
    "    - 加權策略 : 使用TF-IDF發現特徵詞，找出文件主要特徵\n",
    "    - 分類器 : 使用演算法訓練分類器\n",
    "    - 評價分類結果 : 分類器的測試結果分析\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ```中文文字分類步驟一 : 前置處理```\n",
    "\n",
    "- 文字的前置處理又分為以下幾個步驟\n",
    "    - 選擇處理文字的範圍\n",
    "    - 建立文字語料庫\n",
    "        - 訓練集語料\n",
    "        - 測試集語料\n",
    "    - 文字格式轉換\n",
    "    - 檢測邊界:標示句子的結束"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ```中文文字分類步驟二 : 分詞```\n",
    "\n",
    "- 中文分詞一般使用的演算法為\n",
    "    - ```條件隨機場(CRF):這```是一種機率模型\n",
    "- 商用的分詞工具\n",
    "    - [北京理工大學的中文分詞系統](http://ictclas.nlpir.org/)\n",
    "    - [哈爾濱大學的中文分詞系統](http://www.ltp-cloud.com/intro/)\n",
    "    - [Ansj的中文分詞系統](http://www.nlpcn.org/)\n",
    "- ```python中使用的分詞套件為jieba```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ```中文文字分類步驟二 : 分詞(程式範例)```\n",
    "\n",
    "- 對文件中每個文字檔進行分詞程式如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\MIRDEX~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.339 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分詞結束\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree,html \n",
    "import jieba \n",
    "import os\n",
    "\n",
    "corpus_path = \"train_corpus_small/\"   #原始檔案路徑\n",
    "seg_path = \"train_corpus_seg/\"        #處理後的檔案路徑\n",
    "catelist = os.listdir(corpus_path)\n",
    "\n",
    "def readfile(path):\n",
    "    fp = open(path,\"rb\")\n",
    "    content = fp.read()\n",
    "    fp.close()\n",
    "    return content\n",
    "\n",
    "def savefile(savepath,content):\n",
    "    fp = open(savepath,\"wb\")\n",
    "    fp.write(content)\n",
    "    fp.close()\n",
    "\n",
    "for mydir in catelist:\n",
    "    class_path = corpus_path+mydir+\"/\"\n",
    "    seg_dir = seg_path+mydir+\"/\"\n",
    "    if not os.path.exists(seg_dir):\n",
    "        os.makedirs(seg_dir)\n",
    "    file_list = os.listdir(class_path)\n",
    "    for file_path in file_list:\n",
    "        fullname = class_path+file_path\n",
    "        content = readfile(fullname).strip()\n",
    "        content = content.decode().replace(\"\\r\\n\",\"\").strip()\n",
    "        content_seg = jieba.cut(content)\n",
    "        savefile(seg_dir+file_path,\" \".join(content_seg).encode())\n",
    "print(\"分詞結束\")         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ```中文文字分類步驟二 : 分詞```\n",
    "\n",
    "- 分詞後的文字資訊為了後續建立向量空間模型的方便，需將資訊轉為文字向量資訊並物件化(資料儲存的格式化)\n",
    "- 轉換的過程使用```scikit-learn中的bunch```資料結構"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ```中文文字分類步驟二 : 分詞(程式範例)```\n",
    "\n",
    "- 使用```sickit-learn的bunch```資料結構將文字資訊物件化程式如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "建置文字物件結束\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets.base import Bunch\n",
    "import pickle\n",
    "\n",
    "bunch = Bunch(target_name=[],label=[],filenames=[],contents=[]) #宣告物件格式\n",
    "\n",
    "wordbag_path = \"train_word_bag/train_set.dat\"    #格式化之後儲存的路徑，以pickle的方式儲存\n",
    "seg_path = \"train_corpus_seg/\"       #分詞後，要進行格式化的檔案路徑\n",
    "\n",
    "catelist = os.listdir(seg_path)\n",
    "bunch.target_name.extend(catelist)\n",
    "for mydir in catelist:\n",
    "    class_path = seg_path+mydir+\"/\"\n",
    "    file_list = os.listdir(class_path)\n",
    "    for file_path in file_list:\n",
    "        fullname = class_path + file_path\n",
    "        bunch.label.append(mydir)\n",
    "        bunch.filenames.append(fullname)\n",
    "        bunch.contents.append(readfile(fullname).strip())\n",
    "file_obj = open(wordbag_path,\"wb\")\n",
    "pickle.dump(bunch,file_obj)   #以該格式(bunch)狀態(pickle)儲存於硬碟中\n",
    "file_obj.close()\n",
    "print(\"建置文字物件結束\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ```中文文字分類步驟三 : 向量空間模型```\n",
    "\n",
    "- 向量空間模型為文字分類的結構化方法\n",
    "- 向量空間模型將文字表示為一個向量，其特徵為文字中出現的詞\n",
    "- 向量空間模型\n",
    "    - 由於文字向量是一個高維空間，為了降低維度必須先過濾掉停用詞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ```中文文字分類步驟三 : 向量空間模型(程式範例)```\n",
    "\n",
    "- 讀取停用詞檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopword_path = \"train_word_bag\\hlt_stop_words.txt\"\n",
    "stpwrdlst = readfile(stopword_path).splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ```中文文字分類步驟四 : TF-IDF加權策略```\n",
    "\n",
    "- ```詞頻(TF)```\n",
    "    - 指某一指定的詞在文中的出現次數(是一種歸一化的表現)\n",
    "    $$n_{i,j} \\over \\sum\\limits_{k=1}n_{k,j}$$\n",
    "    - 其中$n_{i,j}$表示在文中的出現次數，$n_{k,j}$的加總為該文件中所有詞彙的總和\n",
    "- ```逆向檔案頻率(IDF)```\n",
    "    - 意義:當某一詞彙在該文件中頻繁出現，但在其他文件中較少出現則表示該詞彙具有較高識別能力。應給予較高權重!反之應給予較少權重\n",
    "    $$log{|D| \\over |j:t_i \\in d_j|}$$\n",
    "    - 其中分子D為所有文件個數，分母為該詞在所有文件中的出現次數(一個詞在某文件中即便出現很多次也算一次)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ```中文文字分類步驟四 : TF-IDF加權策略(範例)```\n",
    "\n",
    "- 有三個文件\n",
    "    - ```文件一:My dog ate my homework```\n",
    "    - ```文件二:My cat ate the sandwich```\n",
    "    - ```文件三:A dolphin ate the homework```\n",
    "- 三個文件產生的詞頻\n",
    "    - ```a(1次), ate(3次), cat(1次), dolphin(1次), dog(1次), homework(2次), my(3次), sandwich(1次), the(2次)```\n",
    "- 經詞頻轉換後的三個文件\n",
    "    - 文件一 : ```0, 1, 0, 0, 1, 1, 1, 0, 0```\n",
    "    - 文件二 : ```0, 1, 1, 0, 0, 0, 1, 1, 1```\n",
    "    - 文件三 : ```1, 1, 0, 1, 0, 1, 0, 0, 1```\n",
    "- 上述的詞頻的轉換方式會忽略掉詞彙在文件中出現的次數，因此正確轉換如下\n",
    "    - 文件一 : ```0, 1, 0, 0, 1, 1, 2, 0, 0 (my在文件中出現2次)```\n",
    "    - 文件二 : ```0, 1, 1, 0, 0, 0, 1, 1, 1```\n",
    "    - 文件三 : ```1, 1, 0, 1, 0, 1, 0, 0, 1```\n",
    "- 為了避免句子長度的不一致，因此使用歸一化 ( 這就是TF )\n",
    "    - 文件一 : ```0, 1/5, 0, 0, 1/5, 1/5, 2/5, 0, 0 (my在文件中出現2次)```\n",
    "    - 文件二 : ```0, 1/5, 1/5, 0, 0, 0, 1/5, 1/5, 1/5```\n",
    "    - 文件三 : ```1/5, 1/5, 0, 1/5, 0, 1/5, 0, 0, 1/5```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ```中文文字分類步驟四 : TF-IDF加權策略(範例)```\n",
    "\n",
    "- 詞頻的歸一化(詞頻是所有文件中詞彙出現的次數，因此歸一化分母應擺文件數)\n",
    "    - ```a(1次/3份文件), ate(3次/3份文件), cat(1次/3份文件), dolphin(1次/3份文件), dog(1次/3份文件), homework(2次/3份文件), my(3次/3份文件), sandwich(1次), the(2次/3份文件)```\n",
    "- 逆向檔案頻率(IDF)\n",
    "    - ```a log(3/1), ate log(3/3), cat log(3/1), dolphin log(3/1), dog log(3/1), homework log(3/2), my log(3份文件/2份文件裡有出現), sandwich log(3/1), the log(3/2)```\n",
    "- 最後再計算 TF與IDF 的乘積。TF-IDF偏好過濾掉常見的詞語，保留重要的詞語。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ```中文文字分類步驟四 : TF-IDF加權策略(程式範例)```\n",
    "\n",
    "- 匯入所需Scikit-Learn套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from sklearn.datasets.base import Bunch\n",
    "import pickle\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ```中文文字分類步驟四 : TF-IDF加權策略(程式範例)```\n",
    "\n",
    "- 讀取和寫入Bunch物件的函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 讀取Bunch狀態下的pickle物件\n",
    "\n",
    "def readbunchobj(path):\n",
    "    file_obj = open(path, \"rb\")\n",
    "    bunch = pickle.load(file_obj)\n",
    "    file_obj.close()\n",
    "    return bunch\n",
    "\n",
    "# 寫入Bunch狀態下的pickle物件\n",
    "\n",
    "def writebunchobj(path,bunchobj):\n",
    "    file_obj = open(path, \"wb\")\n",
    "    pickle.dump(bunchobj,file_obj)\n",
    "    file_obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ```中文文字分類步驟四 : TF-IDF加權策略(程式範例)```\n",
    "\n",
    "- 從訓練集產生TF-IDF向量詞袋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 匯入分詞後的詞向量Bunch物件\n",
    "\n",
    "path = \"train_word_bag/train_set.dat\"\n",
    "bunch = readbunchobj(path)\n",
    "# 建置TF-IDF詞向量空間物件\n",
    "\n",
    "tfidfspace = Bunch(target_name=bunch.target_name, label=bunch.label, filenames=bunch.filenames, tdm=[], vocabulary={})\n",
    "\n",
    "# 使用TfidfVectorizer初始化向量空間模型\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stpwrdlst, sublinear_tf=True, max_df=0.5)\n",
    "transformer=TfidfTransformer()     #計算每個詞語的TF-IDF\n",
    "\n",
    "# 文字轉為詞頻矩陣，單獨儲存字典檔案\n",
    "\n",
    "tfidfspace.tdm = vectorizer.fit_transform(bunch.contents)\n",
    "tfidfspace.vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ```中文文字分類步驟四 : TF-IDF加權策略(程式範例)```\n",
    "\n",
    "- 持久化TF-IDF向量詞袋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 建立詞袋的持久化\n",
    "\n",
    "space_path = \"train_word_bag/tfdifspace.dat\"\n",
    "writebunchobj(space_path,tfidfspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `中文文字分類步驟五 : 分類器（1）`\n",
    "\n",
    "- 中文的分類方法有以下幾種\n",
    "    - kNN最近鄰演算法 : 原理簡單，分類精度尚可，但速度很慢。\n",
    "    - 單純貝氏分類 : 分類短文字效果好，精度高。\n",
    "    - 支援向量機 : 支援線性不可分的情況，精度尚可。\n",
    "- 這次的實驗將使用Scikit-Learn中的單純貝氏演算法進行分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `中文文字分類步驟五 : 分類器（2）`\n",
    "\n",
    "- 在使用分類器建模之前，必須先對測試資料進行整理。整理的方式與訓練資料相同\n",
    "- 首先先將已經分詞好的測試資料使用Bunch結構化並以Pickle儲存於本機"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "建置文字物件結束\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets.base import Bunch\n",
    "import pickle\n",
    "\n",
    "bunch = Bunch(target_name=[],label=[],filenames=[],contents=[]) #宣告物件格式\n",
    "\n",
    "wordbag_path = \"test_word_bag/test_set.dat\"    #格式化之後儲存的路徑，以pickle的方式儲存\n",
    "seg_path = \"test_corpus_seg/\"       #分詞後，要進行格式化的檔案路徑\n",
    "\n",
    "catelist = os.listdir(seg_path)\n",
    "bunch.target_name.extend(catelist)\n",
    "for mydir in catelist:\n",
    "    class_path = seg_path+mydir+\"/\"\n",
    "    file_list = os.listdir(class_path)\n",
    "    for file_path in file_list:\n",
    "        fullname = class_path + file_path\n",
    "        bunch.label.append(mydir)\n",
    "        bunch.filenames.append(fullname)\n",
    "        bunch.contents.append(readfile(fullname).strip())\n",
    "file_obj = open(wordbag_path,\"wb\")\n",
    "pickle.dump(bunch,file_obj)   #以該格式(bunch)狀態(pickle)儲存於硬碟中\n",
    "file_obj.close()\n",
    "print(\"建置文字物件結束\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `中文文字分類步驟五 : 分類器（3）`\n",
    "- 將Bunch物件化的測試資料集對映到訓練資料集的詞袋，並建置TF-IDF向量空間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 匯入分詞後的詞向量Bunch物件\n",
    "path = \"test_word_bag/test_set.dat\"\n",
    "bunch = readbunchobj(path)\n",
    "#建置測試集TF-IDF向量空間\n",
    "testspace = Bunch(target_name=bunch.target_name, label=bunch.label, filenames=bunch.filenames, tdm=[], vocabulary={})\n",
    "#匯入訓練及詞袋\n",
    "trainbunch = readbunchobj(\"train_word_bag/tfdifspace.dat\")\n",
    "#使用TfidfVectorizer初始化向量空間模型\n",
    "vectorizer = TfidfVectorizer(stop_words=stpwrdlst, sublinear_tf=True, max_df=0.5, vocabulary=trainbunch.vocabulary)#使用訓練集詞袋\n",
    "transformer=TfidfTransformer()\n",
    "testspace.tdm = vectorizer.fit_transform(bunch.contents)\n",
    "testspace.vocabulary = trainbunch.vocabulary\n",
    "#建立詞袋持久化\n",
    "space_path = \"test_word_bag/testspace.dat\"\n",
    "writebunchobj(space_path,testspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `中文文字分類步驟五 : 分類器（4）`\n",
    "\n",
    "- 執行貝氏分類，對測試文字進行分類，並回傳分類精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# 匯入資料集向量空間\n",
    "trainpath = \"train_word_bag/tfdifspace.dat\"\n",
    "train_set = readbunchobj(trainpath)\n",
    "# 匯入測試集向量空間\n",
    "testpath = \"test_word_bag/testspace.dat\"\n",
    "test_set = readbunchobj(testpath)\n",
    "# 應用單純貝氏演算法(alpha:0.001，alpha越小反覆運算次數越多，精度越高)\n",
    "clf = MultinomialNB(alpha = 0.001).fit(train_set.tdm, train_set.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `中文文字分類步驟五 : 分類器（5）`\n",
    "- 預測分類結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_corpus_seg/art/3143.txt :實際類別: art -->預測類別 education\n",
      "error rate: 0.9900990099009901 %\n"
     ]
    }
   ],
   "source": [
    "#預測分類結果\n",
    "predicted = clf.predict(test_set.tdm)\n",
    "total = len(predicted)\n",
    "rate=0\n",
    "for flabel,file_name,expct_cate in zip(test_set.label,test_set.filenames,predicted):\n",
    "    if flabel != expct_cate:\n",
    "        rate+=1\n",
    "        print(file_name,\":實際類別:\",flabel,\"-->預測類別\",expct_cate)\n",
    "print(\"error rate:\",float(rate)*100/float(total),\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `中文文字分類步驟六 : 評估分類結果（1）`\n",
    "\n",
    "- 分類模型的評估指標有以下三種\n",
    "    - 召回率`(Recall)` : 所有正類的結果中，預測為正類的比率。即${TP} \\over {TP+FN}$\n",
    "    - 準確率`(Precision)`:所有預測為正類的結果中，確實是正類的比率${TP} \\over {TP+FP}$。\n",
    "    - `F-Measure(F-Score)`:分類評估器的標準價值。即${2\\times P \\times R} \\over {P+R}$，其中`P是Precision、R是Recall`\n",
    "![混淆矩陣](https://mirdex.github.io/Machine_Learning/混淆矩陣.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `中文文字分類步驟六 : 評估分類結果（2）`\n",
    "- 該文字分類的評估值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision：0.991\n",
      "Recall： 0.990\n",
      "F-Measure: 0.990\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "def metrics_result(actual,predict):\n",
    "    print (\"Precision：%.3f\" % metrics.precision_score(actual,predict,average='weighted'))\n",
    "    print(\"Recall： %.3f\" % metrics.recall_score(actual,predict,average='weighted'))\n",
    "    print(\"F-Measure: %.3f\" % metrics.f1_score(actual,predict,average='weighted'))\n",
    "\n",
    "metrics_result(test_set.label,predicted)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
