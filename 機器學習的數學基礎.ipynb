{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 機器學習的數學基礎\n",
    "\n",
    "## ```Da-Wei Chiang```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 相似性的度量\n",
    "\n",
    "- 相似度的定義\n",
    "    - 每個物件```(人或商品或...)都是n```維空間中的點，通過計算點與點之間的距離來度量其相似性。\n",
    "- 相似度的計算方式\n",
    "    - 透過計算兩向量之間的距離```(Distance)```來度量樣本之間的相似度```(Similarity)```，距離越進則越相似 ; 距離越遠越不相同。\n",
    "- 各種計算距離的方式\n",
    "    - 歐式距離```(Euclidean Distance)```\n",
    "    - 曼哈頓距離```(Manhattan Distance)```\n",
    "    - 謝比雪夫距離```(Chebyshev Distance)```\n",
    "    - 夾角餘弦```(Cosine)```\n",
    "    - 傑卡德相似係數```(Jaccard Similarity Coefficient)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 歐式距離```(Euclidean Distance)```\n",
    "\n",
    "- 在歐式空間中，兩點之間的距離公式如下\n",
    "    - 在二維平面上```a(x1,y1)與b(x2,y2)```間的歐氏距離為：\n",
    "$$d_{12} = \\sqrt{(x_{1} - x_{2})^2 + (y_1 - y_2)^2}$$\n",
    "\n",
    "    - 3D空間的兩點A(x1,y1,z1)與B(x2,y2,z2)間的歐氏距離為：\n",
    "$$d_{12} = \\sqrt{(x_{1} - x_{2})^2 + (y_1 - y_2)^2 + (z_1 - z_2)^2}$$\n",
    "\n",
    "    - n維空間下A(x11,x12,...,x1n)與B(x21,x22,...,x2n)間的歐氏距離為：\n",
    "$$d_{12} = \\sqrt{\\sum\\limits_{k=1}^{n}(x_{1k}-x_{2k})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 曼哈頓距離```(Manhattan Distance)```\n",
    "\n",
    "- 曼哈頓距離，兩點之間的距離公式如下\n",
    "    - 在二維平面上```a(x1,y1)與b(x2,y2)```間的歐氏距離為：\n",
    "    $$d_{12} = |x_1-x_2|+|y_1-y_2|$$\n",
    "    - n維空間下A(x11,x12,...,x1n)與B(x21,x22,...,x2n)間的歐氏距離為：\n",
    "$$d_{12} = \\sum\\limits_{k=1}^n|x_{1k}-x_{2k}|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 謝比雪夫距離```(Chebyshev Distance)```\n",
    "\n",
    "- 謝比雪夫距離，兩點之間的距離公式如下\n",
    "    - 在二維平面上A(x1,y1)與B(x2,y2)間的歐氏距離為：\n",
    "    $$d_{12} = max(|x_1-x_2|,|y_1-y_2|)$$\n",
    "    - n維空間下A(x11,x12,...,x1n)與B(x21,x22,...,x2n)間的謝比雪夫距離為：\n",
    "    $$d_{12} = max_{i}(|x_{1i}-x_{2i}|)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 夾角餘弦```(Cosine)```\n",
    "\n",
    "- 夾角餘弦是用來衡量兩個向量方向的差異，機器學習以此概念來衡量向量之間的差異\n",
    "    - 在二維平面上A(x1,y1)與B(x2,y2)間的夾角餘弦公式為：\n",
    "    $$cosθ = {x_1x_2+y_1y_2\\over\\sqrt{x_1^2+y_1^2}\\sqrt{x_2^2+y_2^2}}$$\n",
    "    - n維空間下A(x11,x12,...,x1n)與B(x21,x22,...,x2n)間的夾角餘弦公式為：\n",
    "    $$cosθ = {AB\\over|A||B|} = {{\\sum\\limits_{k=1}^nx_{1k}x_{2k}}\\over{\\sqrt{\\sum\\limits_{k=1}^nx_{1k}^2}\\sqrt{\\sum\\limits_{k=1}^nx_{2k}^2}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 傑卡德相似係數```(Jaccard Similarity Coefficient)```\n",
    "\n",
    "- 傑卡德相關係數\n",
    "    - 兩集合A、B的交集元素在A、B聯集中所佔的比率\n",
    "    $$J(A,B) = {{|A∩B|}\\over{|A∪B|}}$$\n",
    "- 傑卡德距離與傑卡德相關係數的概念相反\n",
    "    - 若A、B兩集合越相似則相關係數越大(距離越小)、若越不相似相關係數越小(距離越大)。因此公式如下\n",
    "    $$J_δ(A,B) = 1 - J(A,B)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 淺談機率論\n",
    "\n",
    "- 機率的兩個主要概念\n",
    "    - 正確性\n",
    "        - 樣本整體的設定值範圍是確定的\n",
    "    - 隨機性\n",
    "        - 雖然確定樣本的設定值範圍，但卻不知道樣本會落於該範圍中的哪個點上\n",
    "- 正確性與隨機性範例\n",
    "    - 火車每次進站的停車位置都在月台附近(正確性)，但實際位置卻都不一樣(隨機性)。\n",
    "    - 拋出的硬幣確定為正面或反面(正確性)，但實際卻不知道其正面還反面(隨機性)。\n",
    "    - 公正的骰每次拋出都會是1~6點(正確性)，但實際卻不知道會是哪個點數(隨機性)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 機率論的概念\n",
    "\n",
    "- 定義：對事物運動不確定性的度量則稱為機率論.\n",
    "- 基礎概念\n",
    "    - 樣本 : 指隨機實驗的結果，如:擲硬幣不是正面就是反面\n",
    "    - 樣本空間 : 隨機實驗中所有結果的集合，如:{正面、反面}\n",
    "    - 隨機事件 : 指樣本空間的子集，如:丟十次硬幣，出現正面的次數\n",
    "    - 隨機變數 : 指某個事件的變數，如:上面範例事件中的隨機變數為X{x = 正面}\n",
    "    - 隨機變數的機率分佈 : 只隨機變數的設定值範圍，某隨機事件出現的可能性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 貝式定理\n",
    "\n",
    "- 貝式定理是機率論中的重要概念，也是機器學習領域中的基礎概念。\n",
    "\n",
    "$$P(B|A) = {P(A|B)P(B) \\over P(A)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 多元統計基礎\n",
    "\n",
    "- 例:蘋果10個其中紅色佔8個、黃色佔2個，梨子10個其中黃色佔9個、綠色佔1個。\n",
    "- 聯合機率分布表如下\n",
    "\n",
    "|顏色\\種類|紅色|黃色|綠色|顏色|\n",
    "|--|---|---|---|---|\n",
    "|蘋果|0.4|0.1|0|0.5|\n",
    "|梨子|0|0.45|0.05|0.5|\n",
    "|水果|0.4|0.55|0.05|1|\n",
    "\n",
    "- 上表中聯合機率分布為\n",
    "$$P\\{X=蘋果，Y=紅色\\} = 0.4　　　P\\{X=梨子，Y=紅色\\} = 0$$\n",
    "$$P\\{X=蘋果，Y=黃色\\} = 0.1　　　P\\{X=梨子，Y=黃色\\} = 0.45$$\n",
    "$$P\\{X=蘋果，Y=綠色\\} =  0  　　　P\\{X=梨子，Y=綠色\\} = 0.05$$\n",
    "- 上表中邊緣機率分布為\n",
    "$$P\\{X=蘋果\\}=0.5　　　P\\{X=梨子\\}=0.5$$\n",
    "$$P\\{X=蘋果\\}=0.5　　　P\\{X=梨子\\}=0.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 矩陣的乘法\n",
    "\n",
    "- 定義 : 矩陣乘法的過程是將一個向量轉換到一個新坐標系(新的空間)的方法\n",
    "- 意義 : 經矩陣乘法後的新向量為新坐標系上的向量，但它與原坐標系上的向量是相等的\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 向量的線性轉換 - 特徵值與特徵向量\n",
    "\n",
    "- 線性轉換的定義 : 轉換的規則是一條通過原點的向量\n",
    "- 再談線性轉換：也因為上述定義的關係。因此不管在任何空間，線性轉換的向量只會在通過原點的線上做伸縮轉換。\n",
    "- 特徵向量與特徵值 : 按照上述的說法，原空間中尚未轉換的向量稱為特徵向量、轉換過後伸縮的比例則稱為特徵值。\n",
    "- 公式如下:(其中A表示原矩陣、v表示特徵向量、$\\lambda$表示特徵值)\n",
    "$$Av = v\\lambda$$\n",
    "- Python實作:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特徵值 : [ 15.           4.89897949  -4.89897949]\n",
      "特徵向量 : [[-0.57735027 -0.81305253 -0.34164801]\n",
      " [-0.57735027  0.47140452 -0.47140452]\n",
      " [-0.57735027  0.34164801  0.81305253]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "\n",
    "A = [[8,1,6],[3,5,7],[4,9,2]]\n",
    "evals, evecs = linalg.eig(A)\n",
    "print(\"特徵值 : %s\" %evals)\n",
    "print(\"特徵向量 : %s\"  %evecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 資料歸一化\n",
    "\n",
    "- 資料歸一化有以下兩種方式\n",
    "    - 將數值變為(0,1)之間的數\n",
    "    - 將有量綱算式變為無量綱算式(一種相對量的比較)\n",
    "        - 例:單純身高、體重沒有可比性，但身高/體重就有可比性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 資料標準化\n",
    "\n",
    "- 標準化說明\n",
    "    - 標準化是歸一化的一種方式\n",
    "- 標準化意義\n",
    "    - 按比例縮放使數值落於某個區間內，使不同度量單位的資料可以相互比較\n",
    "- 標化的方法\n",
    "    - 歐式距離標準化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 歐式距離的標準化\n",
    "\n",
    "- 標準化的方式\n",
    "    - 平均數為0、標準差為1\n",
    "- 標準化公式\n",
    "\n",
    "$$X* = {{X-M} \\over {S}}$$\n",
    "\n",
    "其中 X*=標準化後的值、X=標準化前的值、M=平均數、S=標準差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ```Scikit-Learn```\n",
    "\n",
    "- ```Scikit-Learn是Python```用於機器學習的重要函式庫\n",
    "- ```Scikit-Learn```的模組有意下幾種\n",
    "    - 分類和回歸演算法\n",
    "        - 廣義線性模型、線性和二次判斷分析、嶺回歸、支援向量機、隨機梯度下降、KNN、高斯過程、交換分解、單純貝氏、決策數、整合方法、多細粒度的演算法、特徵選擇、半監督、保序回歸、機率校準\n",
    "    - 分群演算法\n",
    "        - K-Means、仿射傳播、平均值飄移、譜分群、分層分群、DBSCAN、Birch\n",
    "    - 維度約簡\n",
    "        - PCA、潛在語意分析、字典學習、因數分析、ICA、非負矩陣分解\n",
    "    - 模型選擇\n",
    "        - 交換驗證、評價估計效能、網路搜索、搜索參數估計、模型預測品質的量化評價、模型的持久化、驗證曲線、繪製分數評價模型\n",
    "    - 資料前置處理\n",
    "        - 標準化、去除平均值率的方差縮放、正規化、二值化、編分碼類特徵、遺漏值的插補"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
