{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 決策樹\n",
    "\n",
    "## `Da-Wei Chiang`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Outline`\n",
    "\n",
    "- 決策樹的概念\n",
    "- 決策樹的演算法架構\n",
    "- 資訊滳`(Entropy)`\n",
    "- 資訊增益`(Information Gain)`\n",
    "- 演算法\n",
    "    - ID3\n",
    "    - C4.5\n",
    "    - CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 決策樹的概念\n",
    "\n",
    "- 決策樹的概念類似於`if-then`這樣的判斷.\n",
    "- 決策樹共有三種節點，根節點、葉節點、內部節點\n",
    "    - 根節點 : 任選第一個特徵就是根節點；並按照某個規則開始劃分\n",
    "    - 葉節點 : 如果劃分到某個子節點，子節點中所有的樣本為同一類，則該子節點就是葉節點\n",
    "    - 內部節點 : 如果劃分到的子節點仍可繼續劃分，則該子節點稱為內部節點\n",
    "- 決策樹就是這樣不斷地劃分資料，直到畫分到所有資料皆為葉子節點為止"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 決策樹的演算法架構\n",
    "\n",
    "- 主函數\n",
    "- 計算最佳特徵子函數\n",
    "- 劃分資料集函數\n",
    "- 分類器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 決策樹的演算法架構（1）\n",
    "\n",
    "- 主函數\n",
    "    - 決策樹的演算法說穿了就是一個遞迴函數，該函數經由某些規則生長出決策樹的各個節點，並根據終止條件結束演算法\n",
    "- 主函數需要完成以下功能\n",
    "    - 輸入需要分類的資料集\n",
    "    - 根據某種規則獲得最佳的特徵劃分，並建立特徵劃分節點 `(計算最佳特徵子函數)`\n",
    "    - 按照該特徵的特徵職將資料劃分為許多部份 `(劃分資料集子函數)`\n",
    "    - 根據劃分子函數的計算結果建置出新節點，作為樹生長出的新分支\n",
    "    - 檢驗是否符合遞迴的中止條件\n",
    "    - 將劃分新節點的資料集作為輸入，遞迴執行上述步驟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 決策樹的演算法架構（2）\n",
    "\n",
    "- 計算最佳特徵子函數\n",
    "    - 計算最佳特徵子函數是決策樹演算法中最重要的函數。每一種決策樹之所以不同，都是因為選取最佳特徵的標準上有所差異。\n",
    "    - 如 : ID3是使用訊息增益、C4.5是使用訊息增益率、CART是使用節點方差\n",
    "- 劃分資料集函數\n",
    "    - 劃分資料集函數主要的功能是分隔資料集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 決策樹的演算法架構（3）\n",
    "\n",
    "- 分類器\n",
    "    - 決策樹的分類器是透過檢查整個決策樹，使用測試資料找到決策樹中葉子節點所對應的類別標籤。這個標籤就是回傳結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 資訊滳`(Entropy)`（1）\n",
    "\n",
    "- 資訊滳的定義\n",
    "    - 衡量一個空間中資料分布的均勻程度。資料分布的越均勻，滳就越大；反之滳就越小。\n",
    "    - 衡量單一資料的不確定性機率可用$-log (p_i)$\n",
    "    - 但由於資料可能會有多種情況，因此在計算整體資料的不確定性時。應該計算整體不確定性$-log(p_i)$的期望值$E$，也就是資訊滳。其公式如下\n",
    "    $$E[-log p_i] = \\sum\\limits_{i=1}^n(p_i\\ log\\ p_i)$$\n",
    "    - 資訊滳是對於事物不確性的衡量標準"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 資訊增益`(Infomation Gain)`（2）\n",
    "\n",
    "- 首先計算目標類別(假定有個類別值S1、S2)的資訊滳S_I(S1,S2)\n",
    "- 對於每一項特徵項目計算平均資訊期望值，假定某特徵項目I有兩個特徵值(p1、p2)，其平均資訊期望值計算如下\n",
    "    - 先計算p1在整體資料S中佔的比率$p1\\over S$\n",
    "    - 之後計算在p1的情況下S1類的次數C1及p1的情況下S2類的次數C2\n",
    "    - 有了p1情況下各類別次數之後計算p1情況的資訊滳，即$p1\\_I(S1,S2) = I(C1,C2) = -{{C1}\\over p1}log{{C1}\\over p1}-{{C2}\\over p1}log{{C2}\\over p1}$\n",
    "    - 以同樣的步驟計算特徵值p2的資訊滳$p2\\_I(S1,S2)$\n",
    "- 對於該特徵類別有了兩個特徵值的類別資訊滳之後即可計算平均資訊期望值，即\n",
    "$$E(特徵項目) = {p1\\over S} \\times p1\\_I(S1,S2)+{p1\\over S}\\times p1\\_I(S1,S2)$$\n",
    "- 資訊增益\n",
    "$$Information\\ Gain(特徵項目) = S_I(S1,S2) - E(特徵項目)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 演算法 － `ID3`（1）\n",
    "- 我們以一個範例來解釋決策樹演算法\n",
    "- ![決策樹範例](https://mirdex.github.io/Machine_Learning/ID3決策樹範例.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 演算法 － `ID3`（2） \n",
    "\n",
    "- 該範例ID3演算法具體流程放於投影片中\n",
    "    - [ID3流程投影片](https://mirdex.github.io/Machine_Learning/ID3流程簡報.pptx)\n",
    "- 該範例ID3演算法的具體計算流程\n",
    "    - [ID3具體計算流程](https://mirdex.github.io/Machine_Learning/ID3詳細計算表.xlsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 演算法 － `C4.5`（1）\n",
    "\n",
    "- `C4.5`與`ID3`的差別是將資訊增益改為資訊增益率`(Gain Ratio)`作為分割的依據\n",
    "$$GainRatio(S,A) = {{Gain(S,A)}\\over{SplitInfo(S,A)}}$$\n",
    "其中`Gain(S,A)`是`ID3`的資訊增益、`SplitInfo(S,A)`是該資料占整體資料的資訊滳比例\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 演算法 － `C4.5`（2） \n",
    "\n",
    "- 該範例`C4.5`演算法的具體計算流程與ID3相似(只是多了`SplitInfo`、`Gain Ratio`)，因此只計算第一次迭代\n",
    "    - [C4.5流程投影片](https://mirdex.github.io/Machine_Learning/C4.5詳細計算表.xlsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 演算法 － `CART`（1）\n",
    "\n",
    "- 是目前決策樹演算法中最成熟的演算法.\n",
    "- `CART`是一種`Binary Tree`,即每一刀切下去的結果不是「是」就是「否」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 演算法 － `CART`（2）\n",
    "\n",
    "- 範例CART演算法具體流程放於投影片中\n",
    "    - [CART流程投影片](https://mirdex.github.io/Machine_Learning/CART流程簡報.pptx)\n",
    "- 該範例ID3演算法的具體計算流程\n",
    "    - [CART具體計算流程](https://mirdex.github.io/Machine_Learning/CART詳細計算表.xlsx)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
